---
title: "CA_EDA"
author: "Sam Castillo"
date: "January 22, 2018"
output: html_document
---

```{r global_options, warning= F, message= F}
require(lubridate)
library(tidyr)
library(randomForest)
library(e1071)
library(ROCR)
library(pROC)
library(pdp)
library(AppliedPredictiveModeling)
library(mice)
library(caret)
library(tidyverse)#always load last
```

```{r}
load("adapt_clean.Rda")
```

# Feature Engineering

## Missingnes

##Historical Experience

Second attempt using `dplyr`.

```{r}
#re-write these using a function

#not perfect because it counts problems in then-current exam
#this looks back across exams and quizzes
final_adapt.df = final_adapt.df %>%
  group_by(cat1) %>%
  #features for historical major section category 1
  arrange(desc(creation_dt_time)) %>%
  mutate(temp_hist_n_cat1 = ifelse(is.na(cat1), yes = 0, no = cumsum(!is.na(correct)))) %>%
  group_by(cat2) %>%
  #features for historical major section category 1
  arrange(desc(creation_dt_time)) %>%
  mutate(temp_hist_n_cat2 = ifelse(is.na(cat2), yes = 0, no = cumsum(!is.na(correct)))) %>%
  group_by(cat3) %>%
  #features for historical major section category 1
  arrange(desc(creation_dt_time)) %>%
  mutate(temp_hist_n_cat3 = ifelse(is.na(cat3), yes = 0, no = cumsum(!is.na(correct)))) %>%
  #historical features
  mutate(hist_n_cat = temp_hist_n_cat1 + temp_hist_n_cat2 + temp_hist_n_cat3) %>%
  select(-contains("temp")) %>%
  ungroup()
```

Third attempt using general `apply` functions.

```{r}
suppressMessages({
long.df = final_adapt.df %>%
  gather(key = cat_key, value = cat, cat1:cat3, na.rm = T, factor_key = T) %>%
  gather(key = subcat_key, value = subcat, subcat1:subcat3,  na.rm = T, factor_key = T) 
})

get_history = function(examID, creation_dt_time, cat1, cat2, cat3, subcat1, subcat2, subcat3){
  
  cur_examID = examID
  cur_dt_time = creation_dt_time
  cur_cat = unlist(list(cat1, cat2, cat3))
  cur_subcat = unlist(list(subcat1, subcat2, subcat3))

 lookback.list = long.df %>%
   filter(examID != cur_examID, creation_dt_time < cur_dt_time) %>%
   summarise(#categories
             hist_cat_n = sum(ifelse( cat %in% cur_cat, yes = 1, no = 0)),
             hist_cat_ncorrect = sum(ifelse( correct == T & cat %in% cur_cat, yes = 1, no = 0), na.rm = T),
             hist_cat_diff = sum(ifelse( cat %in% cur_cat, yes = difficulty, no = 0)),
             hist_cat_diff_correct = sum(ifelse( correct == T & cat %in% cur_cat, yes = difficulty, no = 0), na.rm = T),
             #subcategories
             hist_subcat_n = sum(ifelse( subcat %in% cur_subcat, yes = 1, no = 0)),
             hist_subcat_ncorrect = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = 1, no = 0), na.rm = T),
             hist_subcat_diff = sum(ifelse( subcat %in% cur_subcat, yes = difficulty, no = 0)),
             hist_subcat_diff_correct = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = difficulty, no = 0), na.rm = T)) %>%
   list()

  lookback.list 
}

#test case
#get_history(final_adapt.df %>% arrange(creation_dt_time) %>% slice(1705)) 

t1 = Sys.time()
completed.df = final_adapt.df %>%
  rowwise() %>%
  mutate(examID = examID,
            hist.list = get_history(examID, creation_dt_time, cat1, cat2, cat3, subcat1, subcat2, subcat3),
            hist_cat_n = hist.list[[1]],
            hist_cat_ncorrect = hist.list[[2]],
            hist_cat_diff = hist.list[[3]],
            hist_cat_diff_correct = hist.list[[4]],
            hist_subcat_n = hist.list[[5]],
            hist_subcat_ncorrect = hist.list[[6]],
            hist_subcat_diff = hist.list[[7]],
            hist_subcat_diff_correct = hist.list[[8]]) %>%
  select(-hist.list)
t2 = Sys.time()
t2 - t1
```


For models to run correctly, there cannot be missing values. 

```{r}
#does this impute the mean without removing the outliers first?  
impute.mean <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), mean(x, na.rm = TRUE))
impute.zero <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), 0)
impute.false = function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), 'FALSE')

#Important note: when I was early on in practicing, I would leave questions NA intentionally instead of guessing.  As I got closer to exam time, I would guess.  For this reason, I choose to replace all NA values in correct with FALSE


model.df = completed.df %>%
  filter(exam_type == "e",
         minutes_used < 30) %>%
  mutate(cat1 = ifelse(is.na(cat1), yes = cat2, no = cat1),
         hrs_since_previous_e_or_q = impute.mean(hrs_since_previous_e_or_q),
         #not ideal; only temporary &&&&&&&&&&
         minutes_used = ifelse(is.na(minutes_used), yes = 0, no = minutes_used),
         approx_remaining_time = impute.mean(approx_remaining_time),
         correct = as.logical(correct)) %>%
         mutate(correct = ifelse(is.na(correct) | is.nan(correct) , yes = F, no = correct))

#weird glitch with dplyr
model.df$correct = as.factor(model.df$correct)

p = model.df %>% filter(course == "P") 
fm = model.df %>% filter(course == "FM") 
mfe = model.df %>% filter(course == "MFE") 
```

#Visualizations

```{r}
x_features = fm %>%
  dplyr::select(difficulty, minutes_used, nth_exam, q_ordinal)
transparentTheme(trans = .9)
caret::featurePlot(x = x_features,
            y = fm$correct,
            plot = "pairs",
            auto.key = list(columns = 2))

```
```{r}
new_x_features = fm %>%
  dplyr::select(difficulty, contains("hist_subcat"))

transparentTheme(trans = .9)

caret::featurePlot(x = new_x_features,
            y = fm$correct,
            plot = "pairs",
            auto.key = list(columns = 2))
```


```{r}
transparentTheme(trans = .9)

featurePlot(x = new_x_features, 
            y = fm$correct,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
             scales = list(x = list(relation="free"), 
                           y = list(relation="free")), 
             adjust = 1.5, 
              pch = "|", 
              # layout = c(4, 2), 
            auto.key = list(columns = 2)
            )
```

```{r}
x_features = featurePlot.df %>%
  dplyr::select( nth_exam, rel_difficulty, minutes_used, q_ordinal, EL_change, hrs_since_previous_e, weekday,hrs_since_previous_e_or_q, creation_hr, marked) %>%
  mutate(nth_exam = as.numeric(nth_exam),
         q_ordinal = as.numeric(q_ordinal),
         #monday = 1, tuesday = 2, etc
         #convert to numeric
         weekday = match(weekday, unique(featurePlot.df$weekday)) + 4)
           
transparentTheme(trans = .9)

featurePlot(x = x_features, 
            y = featurePlot.df$correct,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
             scales = list(x = list(relation="free"), 
                           y = list(relation="free")), 
             adjust = 1.5, 
              pch = "|", 
              layout = c(4, 2), 
            auto.key = list(columns = 2)
            )
```

#Modeling overview

The performance metric is the area under the curve (AUC).

Train/test split:


#Logit model

 - 70% accuracy without using quizz data directly, earned level, or category data


```{r}
f1 = correct ~ difficulty + minutes_used
# define training control
train_control<- trainControl(method="cv", number=10)
# train the model 
logit1<- train(f1, data = p, trControl=train_control, method="glm", family=binomial())

f2 = correct ~ nth_e_or_q + minutes_used  
# train the model 
logit2 <- train(f2, data = p, trControl = train_control, method="glm", family=binomial())
summary(logit2)

f3 = correct ~ nth_e_or_q + minutes_used + hist_n_subcat
# train the model 
logit3 <- train(f3, data = p, trControl = train_control, method="glm", family=binomial())
summary(logit3)
```


```{r}
#slightly better than random guessing
predictor = as.numeric(predict.train(logit1))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```
```{r}
predictor = as.numeric(predict.train(logit2))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```
```{r}
predictor = as.numeric(predict.train(logit3))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```





#Neural Network

#SVM

```{r}
svm1 = svm(correct~., data = rf.data)

svm1
```


#Random Forest 
```{r}
rf.data = model.df %>% 
  filter(course == "FM") %>%
  select(correct, q_ordinal, difficulty, nth_exam,nth_e_or_q, weekday, creation_hr,minutes_used, approx_remaining_time, EL_change,EL_begin, EL_end, hrs_since_previous_e, contains("hist"))%>%
  ungroup() 

rf.data.validation = model.df %>% 
  filter(course == "P") %>%
  select(correct, q_ordinal, difficulty, nth_exam,nth_e_or_q, weekday, creation_hr,minutes_used, approx_remaining_time, EL_change,EL_begin, EL_end,  hrs_since_previous_e, contains("hist"))%>%
  ungroup() 
```

```{r}
rf.data %>% summarise_all(function(dat){sum(is.na(dat)|is.nan(dat)|is.infinite(dat))})
rf.data %>% summarise_all(function(dat){length(levels(dat))})
```

```{r}
# Create model with default paramters
control <- trainControl(method = "repeatedcv", number=10, repeats=3)
seed <- 7; set.seed(seed)
metric <- "Accuracy"
mtry <- sqrt(ncol(rf.data))
tunegrid <- expand.grid(.mtry=mtry)

rf.default <- train(correct ~., data = rf.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)


#Without new features
rf.data.old = rf.data %>% select(-contains("hist"))
rf.old <- train(correct ~., data = rf.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

```
```{r}
rf.default
```

```{r}
varImpPlot(rf2$finalModel)
```


```{r}
#somehow this now works??  The documentation on roxygen is out of date
predictor = as.numeric(predict.train(object = rf.default, new  = rf.data.validation[-1], unkOnly = T))

label = as.numeric(rf.data.validation$correct)
roc(label, predictor)
confusionMatrix(predictor, label)
```

```{r}
predictor = extractPrediction(rf.default,)
label = as.numeric(y.validation)
roc(response, predictor)
confusionMatrix(predictor, label)


```





# Extension to New Data from Quizzes

Test different time allocation patterns and try to maximize the total number of correct questions.

#What features will be different
- Correct will now be a predicted value
- Minutes_used will now be assigned by a rule.  This will be the quantity to maximize over.
- Marked will need to be filled intelligently or not used
- nth_exam can be fixed in the range of 8-10
- weekday can be bootstrapped (or should each simulated exam be on the same day?)
- creation_hr can be bootstrapped

#What Features will Stay the Same
- q_ordinal can be generated as it is random
- difficulty is the same
- nth_e_or_q can be fixed
- cat1 is already fixed
- approx_remaining_time will be determined by the rule
- EL variables can stay the same
- hrs_since_previous can stay the same

```{r}
quiz.df = model.df %>%
  filter(exam_type == "q") %>%
  select(correct, marked, q_ordinal, difficulty, nth_exam, nth_e_or_q, weekday, creation_hr,minutes_used, cat1, approx_remaining_time, EL_change,EL_begin, EL_end, hrs_since_previous_e_or_q, hrs_since_previous_e ) %>%
  as.data.frame()
```

