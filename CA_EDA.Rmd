---
title: "CA_EDA"
author: "Sam Castillo"
date: "January 22, 2018"
output: html_document
---

```{r global_options, warning= F, message= F}
require(lubridate)
library(tidyr)
library(randomForest)
library(e1071)
library(ROCR)
library(pROC)
library(pdp)
library(AppliedPredictiveModeling)
library(mice)
library(Amelia)
library(caret)
library(tidyverse)#always load last
```

```{r}
load("adapt_clean.Rda")
```

## Outlier Removal/Missingness

A quick analysis shows that the time stamp values are missing for the first practice exam in August 24, 2016.  These data have little predictive value as it was my first ever adapt exam.  For this reason, rather than try to impute all of the missing data, I am going to drop it.

```{r}
t = final_adapt.df %>% select(-correct, -contains('cat'))
names = names(t)

t %>% 
  summarise_all(function(dat){sum(is.na(dat)|is.nan(dat)|is.infinite(dat))}) %>% t() %>% as_data_frame() %>%
  mutate(row_names = names,
         n_missing = V1,
         percent_missing = n_missing/nrow(final_adapt.df)) %>%
  filter(n_missing != 0) %>%
  arrange(desc(percent_missing))

missing_obs_index = which(is.na(final_adapt.df$minutes_used)) 
```



```{r}
#modifications
final_adapt.df = final_adapt.df %>% 
#drop missing observations from august 24, 2016
  slice(-missing_obs_index) %>%
  mutate(minutes_used = ifelse(minutes_used > 30 | is.infinite(minutes_used), yes = NA, no = minutes_used))

#high number of problems are at zero as I never started those questions
final_adapt.df %>%
  ggplot(aes(minutes_used)) + 
  geom_histogram()


#part of my model's "accuracy" was from always predicting correct = F when minutes_used = 0.  This was because I hard-coded this to be true.
sum(final_adapt.df$minutes_used==0, na.rm = T)
```
Say that it takes me about 10 seconds to read a problem and decide to skip it.  This means that questions where I spend less than 10 seconds are questions which I never started and should then be excluded.

Most of the zeros are for quizzes, as I would not always finish these.

```{r}
final_adapt.df %>%
  group_by(course, exam_type) %>%
  summarise(percent_zero_or_NA = round(sum(ifelse(minutes_used ==0 | is.na(minutes_used), yes = 1, no = 0)/n()),2),
            percent_zero = round(sum(ifelse(minutes_used ==0 | is.na(minutes_used), yes = 1, no = 0)/n()),2))
```


## Historical Experience Feature Engineering

Third attempt using general `apply` functions.

```{r}
suppressMessages({
long.df = final_adapt.df %>%
  gather(key = cat_key, value = cat, cat1:cat3, na.rm = T, factor_key = T) %>%
  gather(key = subcat_key, value = subcat, subcat1:subcat3,  na.rm = T, factor_key = T) 
})

# 
# get_history = function(examID, creation_dt_time, cat1, cat2, cat3, subcat1, subcat2, subcat3, minutes_used, exam_type){
#   
#   cur_examID = examID 
#   cur_dt_time = creation_dt_time 
#   cur_cat = unlist(list(cat1, cat2, cat3)) 
#   cur_subcat = unlist(list(subcat1, subcat2, subcat3)) 
#   cur_minutes_used = minutes_used 
#   cur_exam_type = exam_type 
#   
#  lookback.list = long.df %>%
#    filter(examID != cur_examID, creation_dt_time < cur_dt_time) %>%
#    summarise(#categories
#              hist_cat_n = sum(ifelse( cat %in% cur_cat, yes = 1, no = 0)),
#              hist_cat_ncorrect = sum(ifelse( correct == T & cat %in% cur_cat, yes = 1, no = 0), na.rm = T),
#              hist_cat_pct_correct = mean(ifelse( correct == T & cat %in% cur_cat, yes = 1, no = 0), na.rm = T),
#              hist_cat_diff = sum(ifelse( cat %in% cur_cat, yes = difficulty, no = 0)),
#              hist_cat_diff_correct = sum(ifelse( correct == T & cat %in% cur_cat, yes = difficulty, no = 0), na.rm = T),
#              #subcategories
#              hist_subcat_n = sum(ifelse( subcat %in% cur_subcat, yes = 1, no = 0)),
#              hist_subcat_ncorrect = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = 1, no = 0), na.rm = T),
#              hist_subcat_pct_correct = mean(ifelse( correct == T & subcat %in% cur_subcat, yes = 1, no = 0), na.rm = T),
#              hist_subcat_diff = sum(ifelse( subcat %in% cur_subcat, yes = difficulty, no = 0)),
#              hist_subcat_diff_correct = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = difficulty, no = 0), na.rm = T),
#              #time features
#              hist_avg_time = mean(ifelse(subcat %in% cur_subcat & exam_type =="e", yes = minutes_used, no = 0)),
#              hist_total_time_e = sum(ifelse(subcat %in% cur_subcat & exam_type =="e", yes = minutes_used, no = 0)),
#              hist_total_time_q = sum(ifelse(subcat %in% cur_subcat & exam_type =="q", yes = minutes_used, no = 0))
#              ) %>%
#    list()
# 
#   lookback.list 
# }

get_history = function(df){
  
  cur_examID = df$examID 
  cur_dt_time = df$creation_dt_time 
  cur_cat = unlist(list(df$cat1, df$cat2, df$cat3)) 
  cur_subcat = unlist(list(df$subcat1, df$subcat2, df$subcat3)) 
  cur_minutes_used = df$minutes_used 
  cur_exam_type = df$exam_type 

 lookback.list = long.df %>%
   filter(examID != cur_examID, creation_dt_time < cur_dt_time) %>%
   summarise(#categories
             hist_cat_n = sum(ifelse( cat %in% cur_cat, yes = 1, no = 0)),
             hist_cat_ncorrect = sum(ifelse( correct == T & cat %in% cur_cat, yes = 1, no = 0), na.rm = T),
             hist_cat_pct_correct = mean(ifelse( correct == T & cat %in% cur_cat, yes = 1, no = 0), na.rm = T),
             hist_cat_diff = sum(ifelse( cat %in% cur_cat, yes = difficulty, no = 0)),
             hist_cat_diff_correct = sum(ifelse( correct == T & cat %in% cur_cat, yes = difficulty, no = 0), na.rm = T),
             #subcategories
             hist_subcat_n = sum(ifelse( subcat %in% cur_subcat, yes = 1, no = 0)),
             hist_subcat_ncorrect = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = 1, no = 0), na.rm = T),
             hist_subcat_pct_correct = mean(ifelse( correct == T & subcat %in% cur_subcat, yes = 1, no = 0), na.rm = T),
             hist_subcat_diff = sum(ifelse( subcat %in% cur_subcat, yes = difficulty, no = 0)),
             hist_subcat_diff_correct = sum(ifelse( correct == T & subcat %in% cur_subcat, yes = difficulty, no = 0), na.rm = T),
             #time features
             hist_avg_time = mean(ifelse(subcat %in% cur_subcat & exam_type =="e", yes = minutes_used, no = 0)),
             hist_total_time_e = sum(ifelse(subcat %in% cur_subcat & exam_type =="e", yes = minutes_used, no = 0)),
             hist_total_time_q = sum(ifelse(subcat %in% cur_subcat & exam_type =="q", yes = minutes_used, no = 0))
             ) %>%
   list()

  lookback.list 
}

#load dplyr again if this fails
t1 = Sys.time()
row.list = split(final_adapt.df, seq(nrow(final_adapt.df)))
#fancy, fancy, fancy vectorized programming
completed.df = cbind(final_adapt.df,bind_rows(sapply(row.list, get_history)))
t2 = Sys.time()

```


For models to run correctly, there cannot be missing values. 

```{r}
#does this impute the mean without removing the outliers first?  
impute.mean <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), mean(x, na.rm = TRUE))
impute.zero <- function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), 0)
impute.false = function(x) replace(x, is.na(x) | is.nan(x) | is.infinite(x), 'FALSE')

#Important note: when I was early on in practicing, I would leave questions NA intentionally instead of guessing.  As I got closer to exam time, I would guess.  For this reason, I choose to replace all NA values in correct with FALSE

model.df = completed.df %>%
  #trainingn the model on practice exams only
  filter(exam_type == "e") %>%
  mutate(cat1 = ifelse(is.na(cat1), yes = cat2, no = cat1),
         correct = as.logical(correct)) %>%
         mutate(correct = ifelse(is.na(correct) | is.nan(correct) , yes = F, no = correct))

#weird glitch with dplyr
model.df$correct = as.factor(model.df$correct)

p = model.df %>% filter(course == "P") 
fm = model.df %>% filter(course == "FM") 
mfe = model.df %>% filter(course == "MFE") 
```

#Visualizations

```{r}
x_features = fm %>%
  dplyr::select(difficulty, minutes_used, nth_exam, q_ordinal)
transparentTheme(trans = .9)
caret::featurePlot(x = x_features,
            y = fm$correct,
            plot = "pairs",
            auto.key = list(columns = 2))

```
```{r}
new_x_features = p %>%
  dplyr::select(difficulty, contains("hist_subcat"), - contains("_diff"))

transparentTheme(trans = .9)

caret::featurePlot(x = new_x_features,
            y = p$correct,
            plot = "pairs",
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")), 
            adjust = 1.5, 
            pch = "|",
            auto.key = list(columns = 2))
```


```{r}
transparentTheme(trans = .9)

featurePlot(x = new_x_features, 
            y = p$correct,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
             scales = list(x = list(relation="free"), 
                           y = list(relation="free")), 
             adjust = 1.5, 
              pch = "|", 
              # layout = c(4, 2), 
            auto.key = list(columns = 2)
            )
```

```{r}
x_features = featurePlot.df %>%
  dplyr::select( nth_exam, rel_difficulty, minutes_used, q_ordinal, EL_change, hrs_since_previous_e, weekday,hrs_since_previous_e_or_q, creation_hr, marked) %>%
  mutate(nth_exam = as.numeric(nth_exam),
         q_ordinal = as.numeric(q_ordinal),
         #monday = 1, tuesday = 2, etc
         #convert to numeric
         weekday = match(weekday, unique(featurePlot.df$weekday)) + 4)
           
transparentTheme(trans = .9)

featurePlot(x = x_features, 
            y = featurePlot.df$correct,
            plot = "density", 
            ## Pass in options to xyplot() to 
            ## make it prettier
             scales = list(x = list(relation="free"), 
                           y = list(relation="free")), 
             adjust = 1.5, 
              pch = "|", 
              layout = c(4, 2), 
            auto.key = list(columns = 2)
            )
```

#Modeling overview

The performance metric is the area under the curve (AUC).

Train/test split:


#Logit model

 - 70% accuracy without using quizz data directly, earned level, or category data


```{r}
f1 = correct ~ difficulty + minutes_used
# define training control
train_control<- trainControl(method="cv", number=10)
# train the model 
logit1<- train(f1, data = p, trControl=train_control, method="glm", family=binomial())

f2 = correct ~ nth_e_or_q + minutes_used  
# train the model 
logit2 <- train(f2, data = p, trControl = train_control, method="glm", family=binomial())
summary(logit2)

f3 = correct ~ nth_e_or_q + minutes_used + hist_subcat_diff_correct
# train the model 
logit3 <- train(f3, data = p, trControl = train_control, method="glm", family=binomial())
summary(logit3)
```


```{r}
#slightly better than random guessing
predictor = as.numeric(predict.train(logit1))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```
```{r}
predictor = as.numeric(predict.train(logit2))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```
```{r}
predictor = as.numeric(predict.train(logit3))
response = as.numeric(p$correct)
roc(response, predictor)
confusionMatrix(response, predictor)
```


#Neural Network

#SVM

```{r}
svm1 = svm(correct~., data = rf.data)

svm1
```


#Random Forest 
```{r}
rf.data = model.df %>% 
  filter(course == "FM") %>%
  select(correct, difficulty, nth_exam,nth_e_or_q, weekday, creation_hr,minutes_used, approx_remaining_time, EL_change,EL_begin, EL_end, hrs_since_previous_e, contains("hist"), -hist_total_time_q)%>%
  ungroup() 

rf.data.validation = model.df %>% 
  filter(course == "P") %>%
  select(correct, q_ordinal, difficulty, nth_exam,nth_e_or_q, weekday, creation_hr,minutes_used, approx_remaining_time, EL_change,EL_begin, EL_end,  hrs_since_previous_e, contains("hist"))%>%
  ungroup() 
```

```{r}
rf.data %>% summarise_all(function(dat){sum(is.na(dat)|is.nan(dat)|is.infinite(dat))})
rf.data %>% summarise_all(function(dat){length(levels(dat))})
```

```{r}
# Create model with default paramters
control <- trainControl(method = "repeatedcv", number=10, repeats=3)
seed <- 7; set.seed(seed)
metric <- "Accuracy"
mtry <- sqrt(ncol(rf.data))
tunegrid <- expand.grid(.mtry=mtry)

rf.default <- train(correct ~., data = rf.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

#Without new features
rf.data.old = rf.data %>% select(-contains("hist"))
rf.old <- train(correct ~., data = rf.data, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)

```
```{r}
rf.default
```

```{r}
varImpPlot(rf.default$finalModel)
```


```{r}
#somehow this now works??  The documentation on roxygen is out of date
predictor = as.numeric(predict.train(object = rf.default, new  = rf.data.validation[-1], unkOnly = T))

label = as.numeric(rf.data.validation$correct)
roc(label, predictor)
confusionMatrix(predictor, label)
```

```{r}
predictor = extractPrediction(rf.default)
label = as.numeric(y.validation)
roc(response, predictor)
confusionMatrix(predictor, label)


```
# K-Nearest Neighbor

```{r}
knn.default <- train(correct ~., data = rf.data, method="knn")
```



# Extension to New Data from Quizzes

Test different time allocation patterns and try to maximize the total number of correct questions.

#What features will be different
- Correct will now be a predicted value
- Minutes_used will now be assigned by a rule.  This will be the quantity to maximize over.
- Marked will need to be filled intelligently or not used
- nth_exam can be fixed in the range of 8-10
- weekday can be bootstrapped (or should each simulated exam be on the same day?)
- creation_hr can be bootstrapped

#What Features will Stay the Same
- q_ordinal can be generated as it is random
- difficulty is the same
- nth_e_or_q can be fixed
- cat1 is already fixed
- approx_remaining_time will be determined by the rule
- EL variables can stay the same
- hrs_since_previous can stay the same

```{r}
quiz.df = model.df %>%
  filter(exam_type == "q") %>%
  select(correct, marked, q_ordinal, difficulty, nth_exam, nth_e_or_q, weekday, creation_hr,minutes_used, cat1, approx_remaining_time, EL_change,EL_begin, EL_end, hrs_since_previous_e_or_q, hrs_since_previous_e ) %>%
  as.data.frame()
```

